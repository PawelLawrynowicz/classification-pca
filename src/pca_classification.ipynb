{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO\n",
    "1. sprawdzić zbalansowanie datasetów; poprawić w razie konieczności\n",
    "2. klasyfikacja; wybrać klasyfikatory i puścić na datasetach\n",
    "3. syntetyczny zbiór danych???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import one_hot\n",
    "from L1PCA import l1pca\n",
    "\n",
    "from sklearn.decomposition import PCA, SparsePCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "from utilities import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(path):\n",
    "    return np.genfromtxt(path, delimiter=\",\")[1:, -1]\n",
    "    \n",
    "def standardize_dataset(path):\n",
    "    data = pd.read_csv(path)\n",
    "    features = list(data.columns[:-1])\n",
    "    x = data.loc[:, features].values\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1pca_transform(path, n):\n",
    "    data = pd.read_csv(path)\n",
    "    features = list(data.columns[:-1])\n",
    "    x = data.loc[:, features].values\n",
    "    principal_components = l1pca(x, n)\n",
    "    return principal_components\n",
    "\n",
    "def pca_transform(path, n):\n",
    "    n = int(n)\n",
    "    standardized_data = standardize_dataset(path)\n",
    "    pca = PCA(n_components=n)\n",
    "    principal_components = pca.fit_transform(standardized_data)\n",
    "    #print(f'EXPLAINED VARIANCE RATIO:\\t{np.around(pca.explained_variance_ratio_, decimals=3)}')\n",
    "    print(f'INFORMATION LOSS:\\t\\t{1-sum(pca.explained_variance_ratio_):.3}')\n",
    "    return principal_components\n",
    "\n",
    "def kernel_pca_transform(path, n):\n",
    "    standardized_data = standardize_dataset(path)\n",
    "    kernel_pca = KernelPCA(n_components=n)\n",
    "    principal_components = kernel_pca.fit_transform(standardized_data)\n",
    "    return principal_components\n",
    "\n",
    "def sparse_pca_transform(path, n):\n",
    "    standardized_data = standardize_dataset(path)\n",
    "    sparse_pca = SparsePCA(n_components=n)\n",
    "    principal_components = sparse_pca.fit_transform(standardized_data)\n",
    "    return principal_components\n",
    "\n",
    "def lda_transform(path, n):\n",
    "    data = pd.read_csv(path)\n",
    "    labels = data['label'].to_numpy()\n",
    "    standardized_data = standardize_dataset(path)\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n)\n",
    "    principal_components = lda.fit_transform(standardized_data, labels)\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYNTHETIC_PATH1 = \"../data/synthetic1.csv\"\n",
    "SYNTHETIC_PATH2 = \"../data/synthetic2.csv\"\n",
    "SYNTHETIC_PATH3 = \"../data/synthetic3.csv\"\n",
    "\n",
    "BREAST_PATH = '../data/breast.csv'\n",
    "HR_PATH = '../data/human_resources.csv'\n",
    "SONAR_PATH = '../data/sonar.csv'\n",
    "SPAM_PATH = '../data/spam.csv'\n",
    "SMOKING_PATH = '../data/smoking.csv'\n",
    "\n",
    "dataset_paths = [SYNTHETIC_PATH1, SYNTHETIC_PATH2, SYNTHETIC_PATH3] #SMOKING_PATH, SONAR_PATH, HR_PATH]\n",
    "\n",
    "methods = {\n",
    "    'pca': pca_transform,\n",
    "    #'kernel_pca': kernel_pca_transform,\n",
    "    'sparse_pca': sparse_pca_transform,\n",
    "    'lda': lda_transform\n",
    "} # l1pca\n",
    "\n",
    "clfs = {\n",
    "    'GNB': GaussianNB(),\n",
    "    'SVM': SVC(),\n",
    "    'kNN': KNeighborsClassifier(),\n",
    "    'CART': DecisionTreeClassifier(random_state=1410),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSyntheticDataset(n_features, n_informative, n_redundant, n_repeated, n_classes, dataset_id=None):\n",
    "    X, y = datasets.make_classification(\n",
    "        n_samples=100,\n",
    "        n_features=n_features,\n",
    "        n_redundant=n_redundant,\n",
    "        n_informative=n_informative,\n",
    "        n_repeated=n_repeated,\n",
    "        random_state=1410,\n",
    "        n_classes=n_classes,\n",
    "        n_clusters_per_class=1\n",
    "    )\n",
    "    dfX = pd.DataFrame(X, columns=[f'f{i}' for i in range(1,X.shape[1] + 1)])\n",
    "    dfX = np.round(dfX, decimals= 5)\n",
    "    dfY = pd.DataFrame(y, columns=['label'])\n",
    "\n",
    "    df = pd.concat([dfX, dfY], axis = 1)\n",
    "    df.to_csv('../data/synthetic'+str(dataset_id)+\".csv\", index = False)\n",
    "\n",
    "# X = pca_transform(SYNTHETIC_PATH, 15)\n",
    "# y = getLabels(SYNTHETIC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spam base dataset\n",
    "\"\"\"\n",
    "def init_spam():\n",
    "    dataset_name = \"spam\"\n",
    "    spam_features = ['word_freq_make','word_freq_address','word_freq_all','word_freq_3d','word_freq_our','word_freq_over','word_freq_remove','word_freq_internet','word_freq_order','word_freq_mail','word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses','word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your','word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george','word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data','word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm','word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project','word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(','char_freq_[','char_freq_!','char_freq_$','char_freq_#','capital_run_length_average','capital_run_length_longest','capital_run_length_total','label']\n",
    "    spam_dataset = pd.read_csv(f\"../raw_data/{dataset_name}.csv\", header=None)\n",
    "    spam_dataset.columns=spam_features\n",
    "    spam_dataset.to_csv(SPAM_PATH, index = False)\n",
    "\n",
    "\"\"\"\n",
    "Sonar dataset\n",
    "\"\"\"\n",
    "def init_sonar():\n",
    "    dataset_name = \"sonar\"\n",
    "    sonar_dataset = pd.read_csv(f\"../raw_data/{dataset_name}.csv\", names = range(0,61))\n",
    "    sonar_dataset.rename(columns={60:'label'}, inplace=True)\n",
    "    # sonar_dataset['label'].replace(0, 'R',inplace=True)\n",
    "    # sonar_dataset['label'].replace(1, 'M',inplace=True)\n",
    "    sonar_dataset.to_csv(SONAR_PATH, index = False)\n",
    "\n",
    "\"\"\"\n",
    "Smoking dataset\n",
    "\"\"\"\n",
    "def init_smoking():\n",
    "    smoking_dataset = pd.read_csv(f\"../raw_data/smoking.csv\")\n",
    "    smoking_dataset = smoking_dataset.drop(['ID'], axis=1)\n",
    "    smoking_dataset_label = smoking_dataset.pop('smoking')\n",
    "    smoking_dataset = one_hot(smoking_dataset, 'gender')\n",
    "    smoking_dataset = one_hot(smoking_dataset, 'oral')\n",
    "    smoking_dataset.rename(columns={'Y':'oral'}, inplace=True)\n",
    "    smoking_dataset = one_hot(smoking_dataset, 'tartar')\n",
    "    smoking_dataset.rename(columns={'Y':'tartar'}, inplace=True)\n",
    "    smoking_dataset.rename(columns={'N':'no_tartar'}, inplace=True)\n",
    "    smoking_dataset.insert(len(smoking_dataset.columns), 'label', smoking_dataset_label)\n",
    "    smoking_dataset.to_csv(SMOKING_PATH, index = False)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Human resources dataset\n",
    "\"\"\"\n",
    "def init_hr():\n",
    "    dataset_name = \"human_resources\"\n",
    "    hr_dataset = pd.read_csv(f\"../raw_data/{dataset_name}.csv\")\n",
    "    hr_labels = hr_dataset.pop('left')\n",
    "    hr_dataset.rename(columns={'sales':'department'}, inplace=True)\n",
    "    hr_dataset = one_hot(hr_dataset, 'salary')\n",
    "    hr_dataset = one_hot(hr_dataset, 'department')\n",
    "    hr_dataset.insert(len(hr_dataset.columns), 'label', hr_labels)\n",
    "    hr_dataset.to_csv(HR_PATH, index = False)\n",
    "    \n",
    "\"\"\"\n",
    "Breast dataset\n",
    "\"\"\"\n",
    "def init_breast():\n",
    "    breast_dataset = pd.DataFrame(pd.read_csv(f\"../raw_data/breast.csv\"))\n",
    "    breast_labels = breast_dataset.pop('diagnosis')\n",
    "    breast_dataset.drop(columns=(['id', 'Unnamed: 32']), inplace=True)\n",
    "    breast_dataset.insert(len(breast_dataset.columns), 'label', breast_labels)\n",
    "    breast_dataset['label'].replace('B', 0,inplace=True)\n",
    "    breast_dataset['label'].replace('M', 1,inplace=True)\n",
    "    breast_dataset.to_csv(BREAST_PATH, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_hr()\n",
    "# init_smoking()\n",
    "# init_breast()\n",
    "# init_spam()\n",
    "# init_sonar()\n",
    "genSyntheticDataset(n_features=20, n_informative=2, n_redundant=2, n_repeated=2, n_classes=4, dataset_id=1)\n",
    "genSyntheticDataset(n_features=40, n_informative=5, n_redundant=10, n_repeated=5, n_classes=10, dataset_id=2)\n",
    "genSyntheticDataset(n_features=60, n_informative=10, n_redundant=20, n_repeated=10, n_classes=2, dataset_id=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_plots(dimensions):\n",
    "\n",
    "    if(dimensions==3):\n",
    "        subplot_kw = dict(projection='3d')\n",
    "    else:\n",
    "        subplot_kw = None\n",
    "    \n",
    "    fig, axs = plt.subplots(len(dataset_paths), len(methods), subplot_kw=subplot_kw, figsize=(25,25))\n",
    "\n",
    "    for row, path in enumerate(dataset_paths):\n",
    "        dataset = pd.read_csv(path)\n",
    "        targets = dataset[dataset.columns[-1]].unique()\n",
    "        colors = ['r', 'b', 'g']\n",
    "\n",
    "        lda_max_dims = len(targets) - 1\n",
    "\n",
    "        for column, method in enumerate(methods):\n",
    "            if row == 0:\n",
    "                axs[row,column].set_title(method)\n",
    "\n",
    "            print(f\"{method} --- --- --- {path}\")\n",
    "\n",
    "            if method == 'lda':\n",
    "                df_dims = lda_max_dims\n",
    "            else:\n",
    "                df_dims = dimensions\n",
    "\n",
    "            components = ([f'principal component {i+1}' for i in range(df_dims)])\n",
    "\t\t\t\n",
    "            dataframe = pd.DataFrame(data = methods[method](path, df_dims), columns=components)\n",
    "            \n",
    "            for (target, color) in zip(targets,colors):\n",
    "\n",
    "                plt.text(0, 1, path, ha='left', va='top', transform=axs[row, column].transAxes)\n",
    "\n",
    "                indicesToKeep = pd.read_csv(path)[dataset.columns[-1]] == target\n",
    "                if df_dims == 1:\n",
    "                    x = dataframe.loc[indicesToKeep, 'principal component 1']\n",
    "                    axs[row, column].scatter(x = x, y = np.zeros_like(x), c = color, s = 40)\n",
    "                elif df_dims == 2:\n",
    "                    axs[row, column].scatter(x = dataframe.loc[indicesToKeep, 'principal component 1'], y = dataframe.loc[indicesToKeep, 'principal component 2'], c = color, s = 40)\n",
    "                elif df_dims == 3:\n",
    "                    axs[row, column].scatter(xs = dataframe.loc[indicesToKeep, 'principal component 1'], ys = dataframe.loc[indicesToKeep, 'principal component 2'], zs = dataframe.loc[indicesToKeep, 'principal component 3'], c = color, s = 40)\n",
    "    \n",
    "    ax_args = {chr(ord('x')+i) + 'label' : 'Principal Component ' + str(i+1) for i in range(0, dimensions)}\n",
    "    for ax in axs.flat:\n",
    "        ax.set(**ax_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "n_datasets = len(dataset_paths)\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "scores = np.zeros((len(clfs), n_datasets, n_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(dataset_paths, method=None, f_ratio=None):\n",
    "\tfolder_location = '../scores/'\n",
    "\tfilename = \"raw\" if method == None and f_ratio == None else method+str(int(f_ratio*100))\n",
    "\tsave_path = folder_location+filename\n",
    "\n",
    "\tfor data_id, dataset in enumerate(dataset_paths):\n",
    "\t\tif method != None and f_ratio != None:\n",
    "\t\t\tdf = pd.read_csv(dataset).to_numpy()\n",
    "\t\t\tn_features = df.shape[1]\n",
    "\t\t\tprint(\"PRINCIPAL COMPONENTS:\\t\\t\", int(f_ratio*n_features))\n",
    "\t\t\tX=methods[method](dataset, int(f_ratio*n_features))\n",
    "\t\t\ty=getLabels(dataset)\n",
    "\t\telse:\n",
    "\t\t\tdataset = pd.read_csv(dataset).to_numpy()\n",
    "\t\t\tX = dataset[:, :-1]\n",
    "\t\t\ty = dataset[:, -1].astype(int)\n",
    "\t\tfor fold_id, (train, test) in enumerate(skf.split(X, y)):\n",
    "\t\t\tfor clf_id, clf_name in enumerate(clfs):\n",
    "\t\t\t\tclf = clone(clfs[clf_name])\n",
    "\t\t\t\tclf.fit(X[train], y[train])\n",
    "\t\t\t\ty_pred = clf.predict(X[test])\n",
    "\t\t\t\tscores[clf_id, data_id, fold_id] = accuracy_score(y[test], y_pred)\n",
    "\t\t\t\t\n",
    "\tnp.save(save_path, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 TOTAL FEATURES\n",
      "PRINCIPAL COMPONENTS:\t\t 5\n",
      "INFORMATION LOSS:\t\t0.449\n",
      "PRINCIPAL COMPONENTS:\t\t 10\n",
      "INFORMATION LOSS:\t\t0.275\n",
      "PRINCIPAL COMPONENTS:\t\t 15\n",
      "INFORMATION LOSS:\t\t0.162\n",
      "40 TOTAL FEATURES\n",
      "PRINCIPAL COMPONENTS:\t\t 10\n",
      "INFORMATION LOSS:\t\t0.184\n",
      "PRINCIPAL COMPONENTS:\t\t 20\n",
      "INFORMATION LOSS:\t\t0.0582\n",
      "PRINCIPAL COMPONENTS:\t\t 30\n",
      "INFORMATION LOSS:\t\t3.75e-13\n",
      "60 TOTAL FEATURES\n",
      "PRINCIPAL COMPONENTS:\t\t 15\n",
      "INFORMATION LOSS:\t\t0.0199\n",
      "PRINCIPAL COMPONENTS:\t\t 30\n",
      "INFORMATION LOSS:\t\t1.69e-13\n",
      "PRINCIPAL COMPONENTS:\t\t 45\n",
      "INFORMATION LOSS:\t\t2.44e-14\n"
     ]
    }
   ],
   "source": [
    "# original scores; no dimensionality reduction\n",
    "calculate_scores(dataset_paths=dataset_paths)\n",
    "print(\"20 TOTAL FEATURES\")\n",
    "calculate_scores(dataset_paths=dataset_paths, method=\"pca\", f_ratio=0.25)\n",
    "print(\"40 TOTAL FEATURES\")\n",
    "calculate_scores(dataset_paths=dataset_paths, method=\"pca\", f_ratio=0.50)\n",
    "print(\"60 TOTAL FEATURES\")\n",
    "calculate_scores(dataset_paths=dataset_paths, method=\"pca\", f_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1513/617612809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nScores:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmean_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMean scores:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results.npy'"
     ]
    }
   ],
   "source": [
    "scores = np.load('results.npy')\n",
    "print(\"\\nScores:\\n\", scores.shape)\n",
    "mean_scores = np.mean(scores, axis=2).T\n",
    "print(\"\\nMean scores:\\n\", mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranks:\n",
      "[[2. 1. 3. 4.]\n",
      " [4. 1. 2. 3.]]\n",
      "\n",
      "Models:\t\t['GNB', 'SVM', 'kNN', 'CART']\n",
      "Mean ranks:\t[3.  1.  2.5 3.5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "ranks = []\n",
    "for ms in mean_scores:\n",
    "    ranks.append(rankdata(ms).tolist())\n",
    "ranks = np.array(ranks)\n",
    "print(f\"Ranks:\\n{ranks}\")\n",
    "mean_ranks = np.mean(ranks, axis=0)\n",
    "print(f\"\\nModels:\\t\\t{[i for i in clfs]}\")\n",
    "print(f\"Mean ranks:\\t{mean_ranks}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "alfa = .05\n",
    "t_statistic = np.zeros((len(clfs), len(clfs)))\n",
    "p_value = np.zeros((len(clfs), len(clfs)))\n",
    "\n",
    "for i in range(len(clfs)):\n",
    "    for j in range(len(clfs)):\n",
    "        t_statistic[i, j], p_value[i, j] = ttest_rel(ranks.T[i], ranks.T[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "t-statistic:\n",
      "          GNB     SVM     kNN     CART\n",
      "----  ------  ------  ------  -------\n",
      "GNB   nan       2.00    0.33    -0.33\n",
      "SVM    -2.00  nan      -3.00    -5.00\n",
      "kNN    -0.33    3.00  nan     -inf\n",
      "CART    0.33    5.00  inf      nan \n",
      "\n",
      "p-value:\n",
      "          GNB     SVM     kNN    CART\n",
      "----  ------  ------  ------  ------\n",
      "GNB   nan       0.30    0.80    0.80\n",
      "SVM     0.30  nan       0.20    0.13\n",
      "kNN     0.80    0.20  nan       0.00\n",
      "CART    0.80    0.13    0.00  nan\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers = list(clfs.keys())\n",
    "names_column = np.expand_dims(np.array(list(clfs.keys())), axis=1)\n",
    "t_statistic_table = np.concatenate((names_column, t_statistic), axis=1)\n",
    "t_statistic_table = tabulate(t_statistic_table, headers, floatfmt=\".2f\")\n",
    "p_value_table = np.concatenate((names_column, p_value), axis=1)\n",
    "p_value_table = tabulate(p_value_table, headers, floatfmt=\".2f\")\n",
    "print(\"\\nt-statistic:\\n\", t_statistic_table, \"\\n\\np-value:\\n\", p_value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_statistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1513/3884561036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madvantage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw_statistic\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m advantage_table = tabulate(np.concatenate(\n\u001b[1;32m      4\u001b[0m     (names_column, advantage), axis=1), headers)\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAdvantage:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w_statistic' is not defined"
     ]
    }
   ],
   "source": [
    "advantage = np.zeros((len(clfs), len(clfs)))\n",
    "advantage[w_statistic > 0] = 1\n",
    "advantage_table = tabulate(np.concatenate(\n",
    "    (names_column, advantage), axis=1), headers)\n",
    "print(\"\\nAdvantage:\\n\", advantage_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical significance (alpha = 0.05):\n",
      "         GNB    SVM    kNN    CART\n",
      "----  -----  -----  -----  ------\n",
      "GNB       0      0      0       0\n",
      "SVM       0      0      0       0\n",
      "kNN       0      0      0       1\n",
      "CART      0      0      1       0\n"
     ]
    }
   ],
   "source": [
    "significance = np.zeros((len(clfs), len(clfs)))\n",
    "significance[p_value <= alfa] = 1\n",
    "significance_table = tabulate(np.concatenate(\n",
    "    (names_column, significance), axis=1), headers)\n",
    "print(\"\\nStatistical significance (alpha = 0.05):\\n\", significance_table)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
